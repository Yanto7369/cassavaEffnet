{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:18.177561Z",
     "iopub.status.busy": "2021-01-28T23:51:18.175882Z",
     "iopub.status.idle": "2021-01-28T23:51:18.178202Z",
     "shell.execute_reply": "2021-01-28T23:51:18.178618Z"
    },
    "papermill": {
     "duration": 0.014865,
     "end_time": "2021-01-28T23:51:18.178753",
     "exception": false,
     "start_time": "2021-01-28T23:51:18.163888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCS_PATH = '/kaggle/input/cassava-leaf-disease-classification'\n",
    "#../input/cassava-leaf-disease-classification/train_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:18.211367Z",
     "iopub.status.busy": "2021-01-28T23:51:18.200831Z",
     "iopub.status.idle": "2021-01-28T23:51:23.054183Z",
     "shell.execute_reply": "2021-01-28T23:51:23.055353Z"
    },
    "papermill": {
     "duration": 4.869576,
     "end_time": "2021-01-28T23:51:23.055538",
     "exception": false,
     "start_time": "2021-01-28T23:51:18.185962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import functools\n",
    "_KERAS_BACKEND = None\n",
    "_KERAS_LAYERS = None\n",
    "_KERAS_MODELS = None\n",
    "_KERAS_UTILS = None\n",
    "def inject_keras_modules(func):\n",
    "    import keras\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        kwargs['backend'] = keras.backend\n",
    "        kwargs['layers'] = keras.layers\n",
    "        kwargs['models'] = keras.models\n",
    "        kwargs['utils'] = keras.utils\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def get_submodules_from_kwargs(kwargs):\n",
    "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    models = kwargs.get('models', _KERAS_MODELS)\n",
    "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    for key in kwargs.keys():\n",
    "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "            raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return backend, layers, models, utils\n",
    "def get_vit_submodules_from_kwargs(kwargs):\n",
    "    #backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    #layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    #models = kwargs.get('models', _KERAS_MODELS)\n",
    "    #utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    num_heads =kwargs.get('num_heads', _KERAS_BACKEND)\n",
    "    mlp_dim =kwargs.get('mlp_dim', _KERAS_BACKEND)\n",
    "    dropout =kwargs.get('dropout', _KERAS_BACKEND)\n",
    "    #for key in kwargs.keys():\n",
    "    #    if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "    #        raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return num_heads,mlp_dim,dropout\n",
    "def get_swish(**kwargs):\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    def swish(x):\n",
    "        \"\"\"Swish activation function: x * sigmoid(x).\n",
    "        Reference: [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)\n",
    "        \"\"\"\n",
    "\n",
    "        if backend.backend() == 'tensorflow':\n",
    "            try:\n",
    "                # The native TF implementation has a more\n",
    "                # memory-efficient gradient implementation\n",
    "                return backend.tf.nn.swish(x)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        return x * backend.sigmoid(x)\n",
    "\n",
    "    return swish\n",
    "def get_dropout(**kwargs):\n",
    "    \"\"\"Wrapper over custom dropout. Fix problem of ``None`` shape for tf.keras.\n",
    "    It is not possible to define FixedDropout class as global object,\n",
    "    because we do not have modules for inheritance at first time.\n",
    "    Issue:\n",
    "        https://github.com/tensorflow/tensorflow/issues/30946\n",
    "    \"\"\"\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    class FixedDropout(keras.layers.Dropout):\n",
    "        def _get_noise_shape(self, inputs):\n",
    "            if self.noise_shape is None:\n",
    "                return self.noise_shape\n",
    "\n",
    "            symbolic_shape = backend.shape(inputs)\n",
    "            noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                           for axis, shape in enumerate(self.noise_shape)]\n",
    "            return tuple(noise_shape)\n",
    "\n",
    "    return FixedDropout\n",
    "def get_weight_crossentropy(**kwargs):\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    def w_categorical_crossentropy(y_true, y_pred,weights,from_logits=False,label_smoothing=0 ):\n",
    "        if(weights==None):\n",
    "            return K.categorical_crossentropy(y_pred, y_true,from_logits,label_smoothing)\n",
    "        nb_cl = len(weights)\n",
    "        final_mask = K.zeros_like(y_pred[:, 0])\n",
    "        y_pred_max = K.max(y_pred, axis=1)\n",
    "        y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "        y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "        for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "              final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "        return K.categorical_crossentropy(y_pred, y_true,from_logits,label_smoothing) * final_mask\n",
    "\n",
    "    class WeightCategoricalCrossentropy( keras.losses.CategoricalCrossentropy):\n",
    "        def __init__(self,\n",
    "                  from_logits=False,\n",
    "                  label_smoothing=0,\n",
    "                  reduction=keras.losses.Reduction.AUTO,\n",
    "                  name='w_categorical_crossentropy',\n",
    "                  weights=None):\n",
    "            @dispatch.add_dispatch_support\n",
    "            def loss_fun():\n",
    "                  return  partial(w_categorical_crossentropy, weights= weights)\n",
    "            super().__init__(\n",
    "            loss_fun,\n",
    "            name=name,\n",
    "            reduction=reduction\n",
    "            )\n",
    "\n",
    "    return  WeightCategoricalCrossentropy\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "class ClassToken(tf.keras.layers.Layer):\n",
    "    \"\"\"Append a class token to an input layer.\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        cls_init = tf.zeros_initializer()\n",
    "        self.hidden_size = input_shape[-1]\n",
    "        self.cls = tf.Variable(\n",
    "            name=\"cls\",\n",
    "            initial_value=cls_init(shape=(1, 1, self.hidden_size), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        cls_broadcasted = tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size])\n",
    "        return tf.concat([cls_broadcasted, inputs], 1)\n",
    "\n",
    "\n",
    "class AddPositionEmbs(tf.keras.layers.Layer):\n",
    "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert (\n",
    "            len(input_shape) == 3\n",
    "        ), f\"Number of dimensions should be 3, got {len(input_shape)}\"\n",
    "        self.pe = tf.Variable(\n",
    "            name=\"pos_embedding\",\n",
    "            initial_value=tf.random_normal_initializer(stddev=0.06)(\n",
    "                shape=(1, input_shape[1], input_shape[2])\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, num_heads=16, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        hidden_size =input_shape[-1]\n",
    "        num_heads = self.num_heads\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {hidden_size} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection_dim = hidden_size // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(hidden_size, name=\"query\")\n",
    "        self.key_dense = tf.keras.layers.Dense(hidden_size, name=\"key\")\n",
    "        self.value_dense = tf.keras.layers.Dense(hidden_size, name=\"value\")\n",
    "        self.combine_heads = tf.keras.layers.Dense(hidden_size, name=\"out\")\n",
    "\n",
    "    # pylint: disable=no-self-use\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output, weights\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Implements a Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, num_heads=16, mlp_dim=4096, dropout=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.att = MultiHeadSelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            name=\"MultiHeadDotProductAttention_1\",\n",
    "        )\n",
    "        self.mlpblock = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(\n",
    "                    self.mlp_dim, activation=tfa.activations.gelu, name=\"Dense_0\"\n",
    "                ),\n",
    "                tf.keras.layers.Dropout(self.dropout),\n",
    "                tf.keras.layers.Dense(input_shape[-1], name=\"Dense_1\"),\n",
    "                tf.keras.layers.Dropout(self.dropout),\n",
    "            ],\n",
    "            name=\"MlpBlock_3\",\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"LayerNorm_0\"\n",
    "        )\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"LayerNorm_2\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x, weights = self.att(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = x + inputs\n",
    "        y = self.layernorm2(x)\n",
    "        y = self.mlpblock(y)\n",
    "        return x + y, weights\n",
    "custom_objects = {\n",
    "        'swish': inject_keras_modules(get_swish)(),\n",
    "        'FixedDropout': inject_keras_modules(get_dropout)(),\n",
    "        'WeightCategoricalCrossentropy':inject_keras_modules(get_weight_crossentropy)(),\n",
    "        'ClassToken': ClassToken,\n",
    "        'AddPositionEmbs': AddPositionEmbs,\n",
    "        'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "}\n",
    "try:\n",
    "        keras.utils.generic_utils.get_custom_objects().update(custom_objects)\n",
    "except AttributeError:\n",
    "        keras.utils.get_custom_objects().update(custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:23.089785Z",
     "iopub.status.busy": "2021-01-28T23:51:23.088996Z",
     "iopub.status.idle": "2021-01-28T23:51:23.095659Z",
     "shell.execute_reply": "2021-01-28T23:51:23.096383Z"
    },
    "papermill": {
     "duration": 0.028632,
     "end_time": "2021-01-28T23:51:23.096563",
     "exception": false,
     "start_time": "2021-01-28T23:51:23.067931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith tf.device('/gpu:0'):\\n    model = tf.keras.models.load_model(modelPath2)\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "modelPath3='/kaggle/input/notebook218606b3bc/Vit_512_best.h5'\n",
    "modelPath4='/kaggle/input/reduve-lr/EffNetB5_512_best (1).h5'\n",
    "modelPath1='/kaggle/input/1efficient/EffNetB5_512_best (3).h5'\n",
    "modelPath2=\"/kaggle/input/densenet2/DenseNet.h5\"\n",
    "model_path_list=[modelPath1,modelPath2,modelPath3,modelPath4]\n",
    "\"\"\"\n",
    "with tf.device('/gpu:0'):\n",
    "    model = tf.keras.models.load_model(modelPath2)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:23.131973Z",
     "iopub.status.busy": "2021-01-28T23:51:23.131144Z",
     "iopub.status.idle": "2021-01-28T23:51:26.594683Z",
     "shell.execute_reply": "2021-01-28T23:51:26.593079Z"
    },
    "papermill": {
     "duration": 3.485685,
     "end_time": "2021-01-28T23:51:26.594832",
     "exception": false,
     "start_time": "2021-01-28T23:51:23.109147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2.0\n",
      "Running on single GPU  /device:GPU:0\n",
      "Number of accelerators:  1\n",
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "import math, re, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "except ValueError:\n",
    "  tpu = None\n",
    "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "\n",
    "if tpu:\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "elif len(gpus) > 1:\n",
    "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
    "elif len(gpus) == 1:\n",
    "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "  print('Running on single GPU ', gpus[0].name)\n",
    "else:\n",
    "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "  print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "GCS_PATH = '/kaggle/input/cassava-leaf-disease-classification'\n",
    "BATCH_SIZE = 1#* strategy.num_replicas_in_sync\n",
    "#BATCH_SIZE=16\n",
    "VALBATCH_SIZE=BATCH_SIZE\n",
    "IMAGE_SIZE = [512, 512]\n",
    "RIMAGE_SIZE=[512//4,512//4]\n",
    "CLASSES = ['0', '1', '2', '3', '4']\n",
    "EPOCHS = 10\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    } if labeled else {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['target'], tf.int32)\n",
    "        return image, label\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
    "    return dataset\n",
    "TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n",
    "    tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/ld_train*.tfrec'),\n",
    "    test_size=0.35, random_state=5\n",
    ")\n",
    "def get_training_dataset(ordered=False):\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:26.621800Z",
     "iopub.status.busy": "2021-01-28T23:51:26.620294Z",
     "iopub.status.idle": "2021-01-28T23:51:26.626120Z",
     "shell.execute_reply": "2021-01-28T23:51:26.625185Z"
    },
    "papermill": {
     "duration": 0.022324,
     "end_time": "2021-01-28T23:51:26.626218",
     "exception": false,
     "start_time": "2021-01-28T23:51:26.603894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n",
    "        tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/ld_train*.tfrec'),\n",
    "        test_size=0.05, random_state=5\n",
    "    )\n",
    "\n",
    "    TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')\n",
    "    def count_data_items(filenames):\n",
    "        n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "        return np.sum(n)\n",
    "    NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:26.668430Z",
     "iopub.status.busy": "2021-01-28T23:51:26.651105Z",
     "iopub.status.idle": "2021-01-28T23:51:26.671832Z",
     "shell.execute_reply": "2021-01-28T23:51:26.671284Z"
    },
    "papermill": {
     "duration": 0.037205,
     "end_time": "2021-01-28T23:51:26.671985",
     "exception": false,
     "start_time": "2021-01-28T23:51:26.634780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "def transform(image,label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = 15. * tf.random.normal([1],dtype='float32')\n",
    "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
    "    res=tf.reshape(d,[DIM,DIM,3])\n",
    "    #res=tf.image.random_brightness(res, 0.2)\n",
    "    return res,label\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')\n",
    "def data_augment(image, label):\n",
    "    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n",
    "    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n",
    "    #image = tf.image.random_flip_left_right(image)\n",
    "    return transform(image,label)#image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:26.693262Z",
     "iopub.status.busy": "2021-01-28T23:51:26.692371Z",
     "iopub.status.idle": "2021-01-28T23:51:26.699705Z",
     "shell.execute_reply": "2021-01-28T23:51:26.700108Z"
    },
    "papermill": {
     "duration": 0.018455,
     "end_time": "2021-01-28T23:51:26.700210",
     "exception": false,
     "start_time": "2021-01-28T23:51:26.681755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_dataset(ordered=True,tta=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    #dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered)\n",
    "    #TRAINING_FILENAMES\n",
    "    if(tta):\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "def to_float32(image, label):\n",
    "    return tf.cast(image, tf.float32), label\n",
    "#testing_dataset = get_test_dataset()\n",
    "#test_ds = get_test_dataset(ordered=True) \n",
    "#test_ds = test_ds.map(to_float32)\n",
    "#testing_dataset = get_test_dataset()\n",
    "#print('Computing predictions...')\n",
    "#test_images_ds = testing_dataset\n",
    "#test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "#probabilities = model.predict(test_images_ds)\n",
    "#predictions = np.argmax(probabilities, axis=-1)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:26.721609Z",
     "iopub.status.busy": "2021-01-28T23:51:26.720968Z",
     "iopub.status.idle": "2021-01-28T23:51:26.726359Z",
     "shell.execute_reply": "2021-01-28T23:51:26.725645Z"
    },
    "papermill": {
     "duration": 0.01773,
     "end_time": "2021-01-28T23:51:26.726483",
     "exception": false,
     "start_time": "2021-01-28T23:51:26.708753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338\n"
     ]
    }
   ],
   "source": [
    "print(count_data_items(VALID_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:51:26.759067Z",
     "iopub.status.busy": "2021-01-28T23:51:26.753605Z",
     "iopub.status.idle": "2021-01-28T23:52:45.106807Z",
     "shell.execute_reply": "2021-01-28T23:52:45.105743Z"
    },
    "papermill": {
     "duration": 78.371127,
     "end_time": "2021-01-28T23:52:45.106932",
     "exception": false,
     "start_time": "2021-01-28T23:51:26.735805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/1efficient/EffNetB5_512_best (3).h5\n",
      "TTA step 1/2\n",
      "TTA step 2/2\n",
      "/kaggle/input/densenet2/DenseNet.h5\n",
      "TTA step 1/2\n",
      "TTA step 2/2\n",
      "/kaggle/input/notebook218606b3bc/Vit_512_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:1030: UserWarning: vit_keras.vit is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA step 1/2\n",
      "TTA step 2/2\n",
      "/kaggle/input/reduve-lr/EffNetB5_512_best (1).h5\n",
      "TTA step 1/2\n",
      "TTA step 2/2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import copy\n",
    "import tensorflow_addons as tfa\n",
    "    \n",
    "with tf.device('/gpu:0'):\n",
    "    CHANNELS = 3\n",
    "    N_CLASSES = 5\n",
    "    TTA_STEPS = 2 # Do TTA if > 0 \n",
    "    #files_path = f'{database_base_path}/test_images/'\n",
    "    test_preds = np.zeros((len(os.listdir(GCS_PATH+'/test_images')), N_CLASSES))\n",
    "    #test_preds = np.zeros((count_data_items(VALID_FILENAMES), N_CLASSES))\n",
    "    \n",
    "\n",
    "    for model_path in model_path_list:\n",
    "        print(model_path)\n",
    "        K.clear_session()\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        test_ds = get_test_dataset(tta=False)\n",
    "        x_test = test_ds.map(lambda image,  idnum: image)\n",
    "        test_preds += model.predict(x_test) / len((TTA_STEPS+1)*model_path_list)\n",
    "\n",
    "        if TTA_STEPS > 0:\n",
    "            test_ds = get_test_dataset( tta=True)\n",
    "            for step in range(TTA_STEPS):\n",
    "                print(f'TTA step {step+1}/{TTA_STEPS}')\n",
    "                x_test = test_ds.map(lambda image,  idnum: image)\n",
    "                #print(x_test.shape)\n",
    "                test_preds+= model.predict(x_test) / ((TTA_STEPS+1) * len(model_path_list))\n",
    "    test_preds = np.argmax(test_preds, axis=-1)\n",
    "    image_names = [img_name.numpy().decode('utf-8') for img, img_name in iter(test_ds.unbatch())]\n",
    "    np.savetxt('submission.csv', np.rec.fromarrays([image_names,test_preds]), fmt=['%s', '%d'], delimiter=',', header='image_id,label', comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-28T23:52:45.137732Z",
     "iopub.status.busy": "2021-01-28T23:52:45.136127Z",
     "iopub.status.idle": "2021-01-28T23:52:45.140180Z",
     "shell.execute_reply": "2021-01-28T23:52:45.139528Z"
    },
    "papermill": {
     "duration": 0.021067,
     "end_time": "2021-01-28T23:52:45.140295",
     "exception": false,
     "start_time": "2021-01-28T23:52:45.119228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 91.773247,
   "end_time": "2021-01-28T23:52:46.184923",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T23:51:14.411676",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
